### 1. 機能要件の整理（現状の確認）

まず、ご提示いただいた内容を「機能要件」として整理します。これは「システムが**何をするか**」を定義するものです。

- **ユーザー認証機能**
  - Gmail による OAuth 認証
  - アカウントのブラックリスト・ホワイトリストによるアクセス制御
- **動画機能**
  - 動画アップロード（3 パターンの事前圧縮）
  - 動画再生（再生・停止、プログレスバー）
  - スワイプによる動画遷移
  - ポートレート（縦）・ランドスケープ（横）表示対応
  - 右スワイプで元動画 URL へ遷移
- **インタラクション機能**
  - いいね
  - リストへの保存（公開・非公開）
  - 共有
- **検索・推薦機能**
  - キーワード検索
  - ハッシュタグ検索
  - 検索条件を固定した状態でのスワイプ
  - スワイプ履歴に基づくハッシュタグの嗜好を学習
  - 嗜好に基づいた動画推薦（レコメンド）
- **マイページ機能**
  - （詳細な要件：例：自分が投稿した動画一覧、いいねした動画一覧、保存リストなど）
- **開発フロー**
  - GitHub Actions による Vercel 風の CI/CD パイプライン

---

### 2. 【最重要】非機能要件の定義

ここが「高品質な大規模システム」の鍵です。「システムが**どのように動くか**」を定義します。

- **パフォーマンス・性能**
  - **レスポンスタイム:** 動画の再生開始まで（Time To First Frame）を何秒以内にしますか？ (例: 1 秒以内)
  - **スワイプ:** 次の動画がバッファリングされ、表示されるまでの時間は？ (例: 500ms 以内)
  - **API 応答:** いいねや保存、検索 API の応答速度は？ (例: 95 パーセンタイルで 200ms 以内)
- **スケーラビリティ（拡張性）**
  - **利用者数:** 将来的にどれくらいのユーザー数（DAU/MAU）を想定しますか？ (例: 100 人？ 1 万人？ 100 万人？)
  - **トラフィック:** 1 秒間にどれくらいのアクセス（RPS）を想定しますか？
  - **データ量:** 1 日にアップロードされる動画の総容量は？
  - **アーキテクチャ:** （後述しますが、Next.js の API Routes と Supabase（Postgres）の構成は、超大規模なトラフィックには限界があります。どのレベルのスケールを目指すかで設計が根本的に変わります）
- **可用性**
  - **稼働率:** 99.9%（年間停止 8.7 時間）を目指しますか？ 99%（年間 3.6 日）で十分ですか？
  - **障害時:** Supabase がダウンした場合、Cloudflare R2 がダウンした場合、アプリはどのように振る舞いますか？（メンテナンス中と表示する、読み取り専用になるなど）
- **セキュリティ**
  - **認証:** OAuth のフローはセキュアに実装されていますか？（CSRF, XSS 対策）
  - **アクセス制御:** 他人の非公開リストや個人情報にアクセスできないことをどう担保しますか？
  - **コンテンツ:** 違法な動画やスパム投稿への対策は？（手動？自動？）
- **法的・コンプライアンス**
  - **著作権:** 著作権侵害（DMCA）の通報と削除フローは必要ですか？
  - **プライバシー:** ユーザーの視聴履歴や個人データをどう保護しますか？（利用規約、プライバシーポリシー）

---

### 3. 「サーバー移行」を見据えた設計（移植性・保守性）

ご提示の「サーバー移行を考慮した」は、**移植性 (Portability)** と**保守性 (Maintainability)** の要件に直結します。

- **ベンダーロックインの回避**
  - **ストレージ (R2):** 非常に良い選択です。R2 は S3 互換 API を持っています。コード上は**S3 の標準 API（AWS SDK など）**を使って R2 に接続するようにしてください。こうすれば、将来的に AWS S3 や MinIO など他の S3 互換ストレージに即座に移行できます。
  - **データベース (Supabase):** 非常に良い選択です。Supabase のバックエンドは**PostgreSQL**です。Supabase 固有の機能（例: Supabase の Auth や Realtime 機能）に依存しすぎると移行が難しくなります。
    - **対策:** データベース操作は可能な限り標準的な SQL（または Prisma のような ORM）で行います。認証ロジックも、Next.js の API 側で JWT の検証を行うなど、Supabase から切り離しやすい設計を心がけると、将来的に別の PostgreSQL（例: Amazon RDS, AlloyDB）への移行が容易になります。
- **フロントエンドとバックエンドの分離**
  - **設計:** Next.js を採用していますが、フロントエンド（Pages/App Router）とバックエンド（API Routes）が密結合になっています。
  - **対策:** これは速度面で利点がありますが、移植性を最優先するなら、API（バックエンド）を別のプロジェクト（例: Hono, NestJS, Go など）として分離し、Cloudflare Workers や Docker コンテナとしてデプロイする設計も考えられます。フロント（Next.js）は API を叩くだけに専念します。
- **コード品質**
  - TypeScript の型定義を厳格にする (strict: true)
  - ESLint, Prettier を CI で強制する
  - コンポーネント設計のルールを定める（Atomic Design, Storybook の導入など）

---

### 4. 運用・開発フロー要件

- **CI/CD (GitHub Actions)**
  - `main`ブランチへのマージ時に Cloudflare Pages の Production 環境へ自動デプロイ
  - Pull Request 作成時に Preview 環境へ自動デプロイ
  - CI パイプラインで、**① 型チェック (tsc) → ② リンター (eslint) → ③ テスト (Jest/Vitest) → ④ ビルド (next build)** を実行し、すべて通らないとマージできないようにする。
- **監視（Monitoring）**
  - API のエラーレート、レスポンスタイムをどう監視しますか？（例: Cloudflare Analytics, Sentry, Logtail）
  - フロントエンドのパフォーマンス（LCP, FID）をどう測定しますか？（Vercel Analytics のような RUM（Real User Monitoring）ツール）
- **ログ（Logging）**
  - API Routes でエラーが発生した際、そのログをどこに集約しますか？（Cloudflare Workers のログ機能、または外部ロギングサービス）

---

### 5. アーキテクチャに関する考察（スケールへの懸念）

「大規模システム」のレベルによりますが、現在の構成（Next.js API + Supabase）は、**中規模（〜数万 MAU）**までは非常に効率的ですが、**TikTok レベル（数億 MAU）**を目指す場合は破綻します。

- **データベース:** 数億の「いいね」や「スワイプ履歴」を単一の PostgreSQL（Supabase）で処理するのは不可能です。大規模システムでは、用途に応じてデータベースを使い分けます（例: ユーザー情報は Postgres, いいねや履歴は Cassandra/ScyllaDB/DynamoDB などの NoSQL）。
- **API:** Next.js の API Routes（サーバーレス関数）は便利ですが、超高トラフィックをさばくにはコストが高くついたり、接続数制限（特に DB）に達したりする可能性があります。
- **レコメンド:** 「スワイプ情報を内部的に持つ」レコメンドエンジンは、PostgreSQL で実行すべき処理ではありません。スワイプログを R2 やメッセージキュー（Cloudflare Queues）に送信し、別のバッチ処理（Spark や Python バッチ）で ML モデルを学習・推論させる専用のマイクロサービスが必要になります。

**結論として：**
まずは現在のスタックで**中規模システム**を想定した非機能要件（例: DAU 1 万人、API 応答 500ms 以内）を定義し、移植性を意識した設計（S3 互換 API の使用、SQL の標準化）で開発を進めるのが現実的かつ高品質なアプローチだと思います。

---

この要件定義のどの部分（例：非機能要件の具体的な数値目標、CI/CD パイプラインの具体的な YAML 設計など）について、さらに詳しく掘り下げますか？
